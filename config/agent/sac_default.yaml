# =============================================================================
# SAC Agent Configuration - Improved for Robotics
# Based on latest research for continuous control tasks
# =============================================================================

# Network architecture - Research-proven settings
architecture:
  # Smaller, more stable network with layer normalization
  hidden_dims: [256, 256]  # Reduced from [512, 512, 256]
  activation: "elu"  # ELU activation works better than ReLU
  layer_norm: true  # CRITICAL: Prevents divergence
  dropout: 0.1  # Small dropout for regularization
  
  # Actor-specific settings
  actor:
    final_activation: "tanh"
    log_std_bounds: [-10, 2]  # Tighter bounds for stability
    mean_range: 1.0
  
  # Critic-specific settings
  critic:
    num_critics: 2
    use_spectral_norm: false

# Learning rates and optimization - Conservative approach
optimization:
  # More conservative learning rates
  lr_actor: 0.0003  # Reduced from 1e-4
  lr_critic: 0.001  # Increased critic learning rate
  lr_alpha: 0.0003  # Matched to actor
  
  # Optimizer settings
  optimizer: "adamw"  # AdamW with weight decay
  weight_decay: 1e-4  # Regularization
  grad_clip_norm: 1.0  # Stricter gradient clipping
  
  # Learning rate scheduling
  lr_schedule:
    enabled: false

# SAC-specific hyperparameters - Tuned for robotics
sac:
  # Core SAC parameters
  gamma: 0.98  # Lower discount factor for shorter-term focus
  tau: 0.005  # Conservative target network updates
  alpha: 0.2  # Higher entropy for more exploration
  automatic_entropy_tuning: true
  target_entropy: -2.0  # Appropriate for 2D action space
  
  # Experience replay - Optimized for simulation
  buffer_size: 1000000  # Smaller buffer for faster learning
  batch_size: 256  # Smaller batches initially
  learning_starts: 2000  # Reduced from 5000
  
  # Training frequency - Research-backed ratios
  train_freq: 10  # Higher frequency to enable gSDE
  gradient_steps: 32  # Multiple gradient steps per update
  
  # Target network updates
  target_update_freq: 1
  
  # Advanced SAC settings
  policy_delay: 4  # Delay policy updates for stability
  target_policy_noise: 0.1  # Lower noise
  target_noise_clip: 0.2  # Tighter clipping

# Exploration and noise - gSDE for consistent exploration
exploration:
  # gSDE (Generalized State Dependent Exploration)
  use_sde: true  # CRITICAL: Enables consistent exploration
  action_noise_type: "gsde"  # Consistent noise over multiple steps
  action_noise_std: 0.1
  sde_sample_freq: 10  # Resample noise every 10 steps
  
  # N-step returns for better learning
  n_steps: 3  # Use 3-step returns
  
  # Exploration schedule
  exploration_schedule:
    enabled: true
    initial_noise: 0.3
    final_noise: 0.05
    decay_steps: 200000
